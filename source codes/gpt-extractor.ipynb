{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import openai\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from pubmed import search as pubmed\r\n",
    "from tqdm import tqdm\r\n",
    "import pickle\r\n",
    "import time\r\n",
    "import glob\r\n",
    "openai.organization = \"org-YqQSvnMXdcSEKapH1nWPeiVh\"\r\n",
    "openai.api_key = \"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\"\r\n",
    "#openai.Model.list()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "pmids_df = pd.read_excel(\"chatgpt-input-data/original_tau_pmids.xlsx\")\r\n",
    "# print(pmids_df)\r\n",
    "pmids = pmids_df[\"pmid\"]\r\n",
    "unique_pmids = list(set(list(pmids)))\r\n",
    "# print(len(unique_pmids))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#Extract abstracts\r\n",
    "abstracts = {}\r\n",
    "errors = []\r\n",
    "for pmid in tqdm(unique_pmids):\r\n",
    "   pubmed_uri  = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/'\r\n",
    "   uri = pubmed_uri + 'efetch.fcgi?db=pubmed&id='+str(pmid)+'&retmode=XML'\r\n",
    "   pdocs = pubmed.fetch_pubmed_by_id(pubmed_uri, str(pmid))\r\n",
    "   # print(\" >>>> title =\", pubmed.get_title_of_document(pdocs, 0))\r\n",
    "   try:\r\n",
    "      abstract = pubmed.get_abstract_of_document(pdocs, 0)\r\n",
    "      #print(\"Type of asbtract is\", type (abstract))        \r\n",
    "      if (type (abstract) is str):\r\n",
    "            text = abstract\r\n",
    "      if isinstance(abstract, list):\r\n",
    "            abstract_ = \"\"\r\n",
    "            # print(\"ordered dictionary is the abstract\")\r\n",
    "            try:\r\n",
    "               for row in abstract:\r\n",
    "                  for key , value in (row.items()):\r\n",
    "                        if (key == '#text'):\r\n",
    "                           abstract_ = abstract_ + value\r\n",
    "               text = abstract_\r\n",
    "               # print(\"Successufully parsed pubmed dictioanry file\")          \r\n",
    "            except :\r\n",
    "               print(\"error while parsing dictionary)\")\r\n",
    "      if isinstance(abstract, bytes):\r\n",
    "          print(pmid)\r\n",
    "          text = abstract.decode()\r\n",
    "      abstracts[pmid] = text\r\n",
    "   except KeyError:\r\n",
    "      print(pmid)\r\n",
    "      continue\r\n",
    "\r\n",
    "\r\n",
    "   \r\n",
    "   # with open('abstracts-checked.pkl', 'wb') as handle:\r\n",
    "   #  pickle.dump(abstracts, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 256/256 [02:34<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Re run on non processed abstracts\r\n",
    "import glob\r\n",
    "import time\r\n",
    "path = r\"rerun\"\r\n",
    "file_list = glob.glob(path + \"/*.txt\")\r\n",
    "for file in tqdm(file_list):\r\n",
    "   file_name = os.path.basename(file)\r\n",
    "   name = os.path.splitext(file_name)[0]\r\n",
    "   messages = [ {\"role\": \"system\", \"content\": 'Extract BEL Triples:\\n- Your task is to extract Biological Expression Language (BEL) triples from provided text.\\n- A BEL triple consists of three components: subject, relation, and object.\\nThe subject and object should be entities categorized into specific namespaces, such as MESH for diseases or biological processes, HGNC for genes, GO for gene functions, and CHEBI for chemicals.\\n- Each subject or object should be preceded by one of the BEL functions: abundance, activity, biologicalProcess, pathology, proteinAbundance, variant.\\n- The relation between subject and object should be chosen from the following list: increase, decrease, positiveCorrelation, negativeCorrelation, analogous, association, biomarkerFor, causesNoChange, directlyDecreases, directlyIncreases, hasActivity, hasComponent, hasComponents, hasMember, hasMembers, increases, isA, orthologous, prognosticBiomarkerFor, rateLimitingStepOf, regulates, subProcessOf, transcribedTo, translatedTo.\\n- Please provide the support evidence by including the sentence that contains the extracted BEL triple.\\n- Use  Namespaces to unambiguously reference concepts. \\n- Associate prefix HGNC for human loci, including protein coding genes, ncRNA genes and pseudogenes.\\n- Associate prefix CHEBI for molecular entities focused on small chemical compounds.\\n- Associate prefix GO to unify the representation of gene and gene product attributes across all species.\\n- Associate prefix MeSH for indexing articles for PubMed.\\n- Associate prefix miRBase which is a biological database that acts as an archive of microRNA sequences and annotations before miRNA names.\\n- Associate prefix MGI which is official gene symbol for a mouse gene.\\n- Associate prefix DO as a standardized ontology for human disease.\\n- Before each subject or object, use one of the BEL functions from the following: abundance, activity, biologicalProcess, pathology, proteinAbundance, variant.\\n\\nExample Triple:\\nTriple1:\\nsubject: biologicalProcess(GO:\"response to fluid shear stress\")\\nrelation: increases\\nobject: rnaAbundance(HGNC:NOS3)\\nsupport evidence: NO is generated by endothelial NO synthase (eNOS, or NOS3), the activity of which is increased by fluid shear stress through various mechanisms.\\n(Note: Ensure that the extracted BEL triples adhere to the specified format and namespaces.)'} ]\r\n",
    "   pmid = name\r\n",
    "   pubmed_uri  = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/'\r\n",
    "   uri = pubmed_uri + 'efetch.fcgi?db=pubmed&id='+str(pmid)+'&retmode=XML'\r\n",
    "   pdocs = pubmed.fetch_pubmed_by_id(pubmed_uri, str(pmid))\r\n",
    "   # print(\" >>>> title =\", pubmed.get_title_of_document(pdocs, 0))\r\n",
    "   try:\r\n",
    "      abstract = pubmed.get_abstract_of_document(pdocs, 0)\r\n",
    "      # print(\"Type of asbtract is\", type (abstract))        \r\n",
    "      if (type (abstract) is str):\r\n",
    "            text = abstract\r\n",
    "      if isinstance(abstract, list):\r\n",
    "            abstract_ = \"\"\r\n",
    "            # print(\"ordered dictionary is the abstract\")\r\n",
    "            try:\r\n",
    "               for row in abstract:\r\n",
    "                  for key , value in (row.items()):\r\n",
    "                        if (key == '#text'):\r\n",
    "                           abstract_ = abstract_ + value\r\n",
    "               text = abstract_\r\n",
    "               print(\"Successufully parsed pubmed dictioanry file\")          \r\n",
    "            except :\r\n",
    "               print(\"error while parsing dictionary)\")\r\n",
    "   except KeyError:\r\n",
    "            print(KeyError)\r\n",
    "   try:\r\n",
    "      time.sleep(2)\r\n",
    "      temperature = 0\r\n",
    "      message = \"Extract all BEL triples for each sentence and provide support evidence for each triple from the following :\" + text            \r\n",
    "      if message:\r\n",
    "            messages.append(\r\n",
    "            {\"role\": \"user\", \"content\": message},)\r\n",
    "            chat = openai.ChatCompletion.create(temperature, model=\"gpt-4\", messages=messages, )\r\n",
    "            ''''''\r\n",
    "      #model=\"gpt-3.5-turbo\"\r\n",
    "      reply = chat.choices[0].message.content\r\n",
    "      messages.append({\"role\": \"assistant\", \"content\": reply})\r\n",
    "      file = open(str(pmid)+\".txt\", \"w\", encoding='utf-8')\r\n",
    "      file.write(reply)\r\n",
    "      file.close()\r\n",
    "   except Exception as e:\r\n",
    "      print(e)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 23%|██▎       | 6/26 [04:14<13:58, 41.93s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successufully parsed pubmed dictioanry file\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 26/26 [20:39<00:00, 47.66s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Test individual abstract\r\n",
    "abstracts = {}\r\n",
    "pmid = 10464280\r\n",
    "pubmed_uri  = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/'\r\n",
    "uri = pubmed_uri + 'efetch.fcgi?db=pubmed&id='+str(pmid)+'&retmode=XML'\r\n",
    "pdocs = pubmed.fetch_pubmed_by_id(pubmed_uri, str(pmid))\r\n",
    "# print(\" >>>> title =\", pubmed.get_title_of_document(pdocs, 0))\r\n",
    "try:\r\n",
    "   abstract = pubmed.get_abstract_of_document(pdocs, 0)\r\n",
    "   # print(\"Type of asbtract is\", type (abstract))        \r\n",
    "   if (type (abstract) is str):\r\n",
    "         text = abstract\r\n",
    "   if isinstance(abstract, list):\r\n",
    "         abstract_ = \"\"\r\n",
    "         # print(\"ordered dictionary is the abstract\")\r\n",
    "         try:\r\n",
    "            for row in abstract:\r\n",
    "               for key , value in (row.items()):\r\n",
    "                     if (key == '#text'):\r\n",
    "                        abstract_ = abstract_ + value\r\n",
    "            text = abstract_\r\n",
    "            # print(\"Successufully parsed pubmed dictioanry file\")          \r\n",
    "         except :\r\n",
    "            print(\"error while parsing dictionary)\")\r\n",
    "   abstracts[pmid] = text\r\n",
    "except KeyError:\r\n",
    "   print(KeyError)\r\n",
    "\r\n",
    "for i in tqdm(range(10)):\r\n",
    "   try:\r\n",
    "      messages = [ {\"role\": \"system\",\r\n",
    " \"content\": 'Extract BEL Triples:\\n- Your task is to extract Biological Expression Language (BEL) triples from provided text.\\n- A BEL triple consists of three components: subject, relation, and object and support sentence.\\nThe subject and object should be entities categorized into specific namespaces, such as MESH for diseases or biological processes, HGNC for genes, GO for gene functions, and CHEBI for chemicals.\\n- Each subject or object should be preceded by one of the BEL functions: abundance, activity, biologicalProcess, pathology, proteinAbundance, variant.\\n- The relation between subject and object should be chosen from the following list: increase, decrease, positiveCorrelation, negativeCorrelation, analogous, association, biomarkerFor, causesNoChange, directlyDecreases, directlyIncreases, hasActivity, hasComponent, hasComponents, hasMember, hasMembers, increases, isA, orthologous, prognosticBiomarkerFor, rateLimitingStepOf, regulates, subProcessOf, transcribedTo, translatedTo.\\n- Please provide the support evidence by including the sentence that contains the extracted BEL triple.\\n- Use  Namespaces to unambiguously reference concepts. \\n- Associate prefix HGNC for human loci, including protein coding genes, ncRNA genes and pseudogenes.\\n- Associate prefix CHEBI for molecular entities focused on small chemical compounds.\\n- Associate prefix GO to unify the representation of gene and gene product attributes across all species.\\n- Associate prefix MeSH for indexing articles for PubMed.\\n- Associate prefix miRBase which is a biological database that acts as an archive of microRNA sequences and annotations before miRNA names.\\n- Associate prefix MGI which is official gene symbol for a mouse gene.\\n- Associate prefix DO as a standardized ontology for human disease.\\n- Before each subject or object, use one of the BEL functions from the following: abundance, activity, biologicalProcess, pathology, proteinAbundance, variant.\\n\\nExample Triple:\\nTriple1:\\nsubject: biologicalProcess(GO:\"response to fluid shear stress\")\\nrelation: increases\\nobject: rnaAbundance(HGNC:NOS3)\\nsupport evidence: NO is generated by endothelial NO synthase (eNOS, or NOS3), the activity of which is increased by fluid shear stress through various mechanisms.\\n(Note: Ensure that the extracted BEL triples adhere to the specified format and namespaces.)'\r\n",
    "} ]\r\n",
    "      temperature = 0\r\n",
    "      abstract = abstracts[pmid]\r\n",
    "      message = \"Extract all BEL triples for each sentence and provide support evidence for each triple from the following :\" + abstract            \r\n",
    "      if message:\r\n",
    "            messages.append(\r\n",
    "            {\"role\": \"user\", \"content\": message},)\r\n",
    "            chat = openai.ChatCompletion.create(top_p = 0.1, model=\"gpt-4\", messages=messages, )\r\n",
    "            ''''''\r\n",
    "      #model=\"gpt-3.5-turbo\"\r\n",
    "      reply = chat.choices[0].message.content\r\n",
    "      print(reply)\r\n",
    "      file = open(str(pmid)+\"prompt3GPT4\"+\"top_p0.1-\"+str(i)+\".txt\", \"w\", encoding='utf-8')\r\n",
    "      file.write(reply)\r\n",
    "      file.close()\r\n",
    "      messages.append({\"role\": \"assistant\", \"content\": reply})\r\n",
    "   except Exception as e:\r\n",
    "      print(e)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 10%|█         | 1/10 [00:37<05:41, 37.89s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Triple1:\n",
      "subject: proteinAbundance(HGNC:MAPT)\n",
      "relation: association\n",
      "object: pathology(DO:\"Alzheimer's disease\")\n",
      "support evidence: Hyperphosphorylated forms of the neuronal microtubule (MT)-associated protein tau are major components of Alzheimer's disease paired helical filaments.\n",
      "\n",
      "Triple2:\n",
      "subject: proteinAbundance(HGNC:PPP2CA)\n",
      "relation: directlyIncreases\n",
      "object: proteinAbundance(HGNC:MAPT)\n",
      "support evidence: Previously, we reported that ABalphaC, the dominant brain isoform of protein phosphatase 2A (PP2A), is localized on MTs, binds directly to tau, and is a major tau phosphatase in cells.\n",
      "\n",
      "Triple3:\n",
      "subject: proteinAbundance(HGNC:PPP2CA)\n",
      "relation: association\n",
      "object: proteinAbundance(HGNC:MAPT)\n",
      "support evidence: We now describe direct interactions among tau, PP2A, and MTs at the submolecular level.\n",
      "\n",
      "Triple4:\n",
      "subject: proteinAbundance(HGNC:PPP2CA)\n",
      "relation: directlyDecreases\n",
      "object: proteinAbundance(HGNC:MAPT)\n",
      "support evidence: Specific PP2A isoforms bind to MTs with distinct affinities in vitro, and these interactions differentially inhibit the ability of PP2A to dephosphorylate various substrates, including tau and tubulin.\n",
      "\n",
      "Triple5:\n",
      "subject: biologicalProcess(GO:\"tubulin assembly\")\n",
      "relation: decreases\n",
      "object: activity(HGNC:PPP2CA)\n",
      "support evidence: Finally, tubulin assembly decreases PP2A activity in vitro, suggesting that PP2A activity can be modulated by MT dynamics in vivo.\n",
      "\n",
      "Triple6:\n",
      "subject: proteinAbundance(HGNC:MAPT)\n",
      "relation: association\n",
      "object: pathology(DO:\"tauopathies\")\n",
      "support evidence: Disruption of these normal interactions could contribute significantly to development of tauopathies such as Alzheimer's disease.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with open('abstracts.pkl', 'rb') as f:\r\n",
    "    abstracts = pickle.load(f)\r\n",
    "\r\n",
    "print(len(abstracts))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "250\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "for i in range(3,6):\r\n",
    "    print(i)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Run on the whole corpora of Tau\r\n",
    "with open('abstracts-checked.pkl', 'rb') as f:\r\n",
    "    abstracts = pickle.load(f)\r\n",
    "cwd = os.getcwd()\r\n",
    "for i in range(5,6):\r\n",
    "    for pmid in tqdm(abstracts.keys()):\r\n",
    "        if os.path.exists('chatgpt-responses/gpt-4/Prompt-2/setting-1/trial-{i_}'.format(i_=i)+'/'+str(pmid)+'.txt'): #Skip already processed docs\r\n",
    "            print(\" >>>> skipping\", pmid, '...')\r\n",
    "        else:\r\n",
    "            try:\r\n",
    "                messages = [ {\"role\": \"system\",\r\n",
    "    \"content\": 'Extract BEL Triples:\\n- Your task is to extract Biological Expression Language (BEL) triples from provided text.\\n- A BEL triple consists of three components: subject, relation, and object.\\nThe subject and object should be entities categorized into specific namespaces, such as MESH for diseases or biological processes, HGNC for genes, GO for gene functions, and CHEBI for chemicals.\\n- Each subject or object should be preceded by one of the BEL functions: abundance, activity, biologicalProcess, pathology, proteinAbundance, variant.\\n- The relation between subject and object should be chosen from the following list: increase, decrease, positiveCorrelation, negativeCorrelation, analogous, association, biomarkerFor, causesNoChange, directlyDecreases, directlyIncreases, hasActivity, hasComponent, hasComponents, hasMember, hasMembers, increases, isA, orthologous, prognosticBiomarkerFor, rateLimitingStepOf, regulates, subProcessOf, transcribedTo, translatedTo.\\n- Please provide the support evidence by including the sentence that contains the extracted BEL triple.\\n- Use  Namespaces to unambiguously reference concepts. \\n- Associate prefix HGNC for human loci, including protein coding genes, ncRNA genes and pseudogenes.\\n- Associate prefix CHEBI for molecular entities focused on small chemical compounds.\\n- Associate prefix GO to unify the representation of gene and gene product attributes across all species.\\n- Associate prefix MeSH for indexing articles for PubMed.\\n- Associate prefix miRBase which is a biological database that acts as an archive of microRNA sequences and annotations before miRNA names.\\n- Associate prefix MGI which is official gene symbol for a mouse gene.\\n- Associate prefix DO as a standardized ontology for human disease.\\n- Before each subject or object, use one of the BEL functions from the following: abundance, activity, biologicalProcess, pathology, proteinAbundance, variant.\\n\\nExample Triple:\\nTriple1:\\nsubject: biologicalProcess(GO:\"response to fluid shear stress\")\\nrelation: increases\\nobject: rnaAbundance(HGNC:NOS3)\\nsupport evidence: NO is generated by endothelial NO synthase (eNOS, or NOS3), the activity of which is increased by fluid shear stress through various mechanisms.\\n(Note: Ensure that the extracted BEL triples adhere to the specified format and namespaces.)'} ]\r\n",
    "                time.sleep(2)\r\n",
    "                # temperature = 0  (using default)\r\n",
    "                top_p = 0.1\r\n",
    "                abstract = abstracts[pmid]\r\n",
    "                message = \"Extract all BEL triples for each sentence and provide support evidence for each triple from the following :\" + abstract         \r\n",
    "                if message:\r\n",
    "                    messages.append(\r\n",
    "                    {\"role\": \"user\", \"content\": message},)\r\n",
    "                    chat = openai.ChatCompletion.create(top_p = .1, model=\"gpt-4\", messages=messages, )\r\n",
    "                    ''''''\r\n",
    "                #model=\"gpt-3.5-turbo\"\r\n",
    "                #model = \"gpt-4\"\r\n",
    "                reply = chat.choices[0].message.content\r\n",
    "                #print(reply)\r\n",
    "                file = open(cwd+\"/chatgpt-responses/gpt-4/Prompt-2/setting-1/trial-{i_}/\".format(i_=i)+str(pmid)+\".txt\", \"w+\", encoding='utf-8')\r\n",
    "                file.write(reply)\r\n",
    "                file.close()\r\n",
    "                messages.append({\"role\": \"assistant\", \"content\": reply})\r\n",
    "                #time.sleep(5)\r\n",
    "            except Exception as e:\r\n",
    "                time.sleep(2)\r\n",
    "                print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "abstracts[23088643]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Run on the whole full texts chunks provided by Juergen\r\n",
    "path_cwd = str(os.getcwd())\r\n",
    "path = path_cwd+\"/fulltext-chunked-Juergen/\"\r\n",
    "print(path)\r\n",
    "file_list = glob.glob(path + \"/*.csv\")\r\n",
    "excl_list = []\r\n",
    "all_triples = []\r\n",
    "not_compiled = []\r\n",
    "for file in tqdm(file_list):\r\n",
    "    #print(file)\r\n",
    "    df = pd.read_csv(file)\r\n",
    "    #print(df)\r\n",
    "    # for col in excel_part.columns:\r\n",
    "    #     print(col)\r\n",
    "    for index, row in df.iterrows():\r\n",
    "        pmcid = row[\"pmcid\"]\r\n",
    "        chunk = row[\"chunk\"]\r\n",
    "        if os.path.exists(path_cwd+'/chatgpt-responses-fulltext/gpt4/trial4/{pmcid_}'.format(pmcid_ = pmcid)+'.txt'): #Skip already processed docs\r\n",
    "            print(\" >>>> skipping\", pmcid, '...')\r\n",
    "        else:\r\n",
    "            try:\r\n",
    "                messages = [ {\"role\": \"system\",\r\n",
    "    \"content\": 'Extract BEL Triples:\\n- Your task is to extract Biological Expression Language (BEL) triples from provided text.\\n- A BEL triple consists of three components: subject, relation, and object.\\nThe subject and object should be entities categorized into specific namespaces, such as MESH for diseases or biological processes, HGNC for genes, GO for gene functions, and CHEBI for chemicals.\\n- Each subject or object should be preceded by one of the BEL functions: abundance, activity, biologicalProcess, pathology, proteinAbundance, variant.\\n- The relation between subject and object should be chosen from the following list: increase, decrease, positiveCorrelation, negativeCorrelation, analogous, association, biomarkerFor, causesNoChange, directlyDecreases, directlyIncreases, hasActivity, hasComponent, hasComponents, hasMember, hasMembers, increases, isA, orthologous, prognosticBiomarkerFor, rateLimitingStepOf, regulates, subProcessOf, transcribedTo, translatedTo.\\n- Please provide the support evidence by including the sentence that contains the extracted BEL triple.\\n- Use  Namespaces to unambiguously reference concepts. \\n- Associate prefix HGNC for human loci, including protein coding genes, ncRNA genes and pseudogenes.\\n- Associate prefix CHEBI for molecular entities focused on small chemical compounds.\\n- Associate prefix GO to unify the representation of gene and gene product attributes across all species.\\n- Associate prefix MeSH for indexing articles for PubMed.\\n- Associate prefix miRBase which is a biological database that acts as an archive of microRNA sequences and annotations before miRNA names.\\n- Associate prefix MGI which is official gene symbol for a mouse gene.\\n- Associate prefix DO as a standardized ontology for human disease.\\n- Before each subject or object, use one of the BEL functions from the following: abundance, activity, biologicalProcess, pathology, proteinAbundance, variant.\\n\\nExample Triple:\\nTriple1:\\nsubject: biologicalProcess(GO:\"response to fluid shear stress\")\\nrelation: increases\\nobject: rnaAbundance(HGNC:NOS3)\\nsupport evidence: NO is generated by endothelial NO synthase (eNOS, or NOS3), the activity of which is increased by fluid shear stress through various mechanisms.\\n(Note: Ensure that the extracted BEL triples adhere to the specified format and namespaces.)'} ]\r\n",
    "                time.sleep(2)\r\n",
    "                # temperature = 0  (using default)\r\n",
    "                top_p = 0.1\r\n",
    "                text = chunk\r\n",
    "                message = \"Extract all BEL triples for each sentence and provide support evidence for each triple from the following :\" + text         \r\n",
    "                if message:\r\n",
    "                    messages.append(\r\n",
    "                    {\"role\": \"user\", \"content\": message},)\r\n",
    "                    chat = openai.ChatCompletion.create(top_p = .1, model=\"gpt-4\", messages=messages, )\r\n",
    "                    ''''''\r\n",
    "                #model=\"gpt-3.5-turbo\"\r\n",
    "                #model = \"gpt-4\"\r\n",
    "                reply = chat.choices[0].message.content\r\n",
    "                #print(reply)\r\n",
    "                file = open(path_cwd+\"/chatgpt-responses-fulltext/gpt4/trial4/{pmcid_}\".format(pmcid_ = pmcid)+\".txt\", \"w+\", encoding='utf-8')\r\n",
    "                file.write(reply)\r\n",
    "                file.close()\r\n",
    "                messages.append({\"role\": \"assistant\", \"content\": reply})\r\n",
    "                time.sleep(5)\r\n",
    "            except Exception as e:\r\n",
    "                time.sleep(5)\r\n",
    "                print(e)\r\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\nbabaiha\\Documents\\GitHub\\chatgpt-paper/fulltext-chunked-Juergen/\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 60%|██████    | 6/10 [5:26:01<3:34:36, 3219.11s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "HTTP code 502 from API (<html>\n",
      "<head><title>502 Bad Gateway</title></head>\n",
      "<body>\n",
      "<center><h1>502 Bad Gateway</h1></center>\n",
      "<hr><center>cloudflare</center>\n",
      "</body>\n",
      "</html>\n",
      ")\n",
      "The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [9:16:56<00:00, 3341.63s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import os\r\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/neginbabaiha/Documents/GitHub/chatgpt-paper'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "!pip install Bio"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: Bio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.5.9)\n",
      "Requirement already satisfied: biopython>=1.80 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Bio) (1.81)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Bio) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Bio) (4.65.0)\n",
      "Requirement already satisfied: mygene in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Bio) (3.2.2)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Bio) (2.0.3)\n",
      "Requirement already satisfied: pooch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Bio) (1.7.0)\n",
      "Requirement already satisfied: gprofiler-official in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Bio) (1.0.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from biopython>=1.80->Bio) (1.25.2)\n",
      "Requirement already satisfied: biothings-client>=0.2.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from mygene->Bio) (0.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/neginbabaiha/Library/Python/3.11/lib/python/site-packages (from pandas->Bio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->Bio) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->Bio) (2023.3)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/neginbabaiha/Library/Python/3.11/lib/python/site-packages (from pooch->Bio) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/neginbabaiha/Library/Python/3.11/lib/python/site-packages (from pooch->Bio) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->Bio) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->Bio) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->Bio) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->Bio) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.5 in /Users/neginbabaiha/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->Bio) (1.16.0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "!pip3 install --upgrade certifi"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2023.7.22)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#Test extract full text\r\n",
    "from Bio import Entrez\r\n",
    "import pandas as pd\r\n",
    "from tqdm import tqdm\r\n",
    "import pickle\r\n",
    "import math\r\n",
    "import time\r\n",
    "df_data = pd.read_excel(\"chatgpt-input-data/initial-tau-triples-pmc-data.xlsx\")\r\n",
    "pmcids = df_data[\"pmc\"]\r\n",
    "pmcids_unique = list(set(pmcids))\r\n",
    "pmcids_unique.pop(0)\r\n",
    "print(len(pmcids_unique))\r\n",
    "for pmcid in tqdm(pmcids_unique):\r\n",
    "    full_texts = {}\r\n",
    "    Entrez.email = \"negin.babaiha@scai.fraunhofer.de\"  # Always tell NCBI who you are\r\n",
    "    handle = Entrez.efetch(db=\"pmc\", id=pmcid,  retmode=\"xml\")\r\n",
    "    response = str(handle.read())\r\n",
    "    full_texts[pmcid] = response\r\n",
    "    #print((response))\r\n",
    "    file = open(\"full-texts/\"+str(pmcid)+ \".txt\",\"w\")\r\n",
    "    file.write(response)\r\n",
    "    file.close()\r\n",
    "    time.sleep(2)\r\n",
    "\r\n",
    "with open('full-texts.pkl', 'wb') as handle:\r\n",
    "    pickle.dump(full_texts, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "142\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 23%|██▎       | 32/142 [01:36<05:30,  3.00s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m full_texts \u001b[39m=\u001b[39m {}\n\u001b[1;32m     15\u001b[0m Entrez\u001b[39m.\u001b[39memail \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnegin.babaiha@scai.fraunhofer.de\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Always tell NCBI who you are\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m handle \u001b[39m=\u001b[39m Entrez\u001b[39m.\u001b[39;49mefetch(db\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpmc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mid\u001b[39;49m\u001b[39m=\u001b[39;49mpmcid,  retmode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mxml\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     17\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(handle\u001b[39m.\u001b[39mread())\n\u001b[1;32m     18\u001b[0m full_texts[pmcid] \u001b[39m=\u001b[39m response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/Bio/Entrez/__init__.py:195\u001b[0m, in \u001b[0;36mefetch\u001b[0;34m(db, **keywords)\u001b[0m\n\u001b[1;32m    193\u001b[0m variables \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdb\u001b[39m\u001b[39m\"\u001b[39m: db}\n\u001b[1;32m    194\u001b[0m variables\u001b[39m.\u001b[39mupdate(keywords)\n\u001b[0;32m--> 195\u001b[0m request \u001b[39m=\u001b[39m _build_request(cgi, variables)\n\u001b[1;32m    196\u001b[0m \u001b[39mreturn\u001b[39;00m _open(request)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/Bio/Entrez/__init__.py:644\u001b[0m, in \u001b[0;36m_build_request\u001b[0;34m(cgi, params, post, ecitmatch, join_ids)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build_request\u001b[39m(cgi, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, post\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ecitmatch\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, join_ids\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    630\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a Request object for an E-utility.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \n\u001b[1;32m    632\u001b[0m \u001b[39m    :param str cgi: base URL for the CGI script to access.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39m    :rtype: urllib.request.Request\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m     params \u001b[39m=\u001b[39m _construct_params(params, join_ids\u001b[39m=\u001b[39;49mjoin_ids)\n\u001b[1;32m    646\u001b[0m     params_str \u001b[39m=\u001b[39m urlencode(params, doseq\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    647\u001b[0m     \u001b[39mif\u001b[39;00m ecitmatch:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/Bio/Entrez/__init__.py:711\u001b[0m, in \u001b[0;36m_construct_params\u001b[0;34m(params, join_ids)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[39m# Format \"id\" parameter properly\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39mif\u001b[39;00m join_ids \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[0;32m--> 711\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _format_ids(params[\u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m params\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/Bio/Entrez/__init__.py:732\u001b[0m, in \u001b[0;36m_format_ids\u001b[0;34m(ids)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mid\u001b[39m\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m ids\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    731\u001b[0m \u001b[39m# Not a string or integer, assume iterable\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mmap\u001b[39m(\u001b[39mstr\u001b[39m, ids))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#test saved full texts\r\n",
    "import pickle\r\n",
    "file = open(\"full-texts.pkl\", \"rb\")\r\n",
    "texts = pickle.load(file)\r\n",
    "file.close()\r\n",
    "file = open(\"full-texts-txt\", \"w\")\r\n",
    "for key, val in texts.items():\r\n",
    "    file.write(str(key))\r\n",
    "    file.write(str(val))\r\n",
    "file.close()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(len(texts))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "#Recheck gpt models on non processed byte TYPE ids\r\n",
    "ids = [17078951, 19363271, 16176937, 24686445, 22817713, 24033439]\r\n",
    "text = \"\"\r\n",
    "for pmid in ids:\r\n",
    "   pubmed_uri  = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/'\r\n",
    "   uri = pubmed_uri + 'efetch.fcgi?db=pubmed&id='+str(pmid)+'&retmode=XML'\r\n",
    "   pdocs = pubmed.fetch_pubmed_by_id(pubmed_uri, str(pmid))\r\n",
    "   # print(\" >>>> title =\", pubmed.get_title_of_document(pdocs, 0))\r\n",
    "   try:\r\n",
    "      abstract = pubmed.get_abstract_of_document(pdocs, 0)\r\n",
    "      #print(\"Type of asbtract is\", type (abstract))        \r\n",
    "      if (type (abstract) is str):\r\n",
    "            text = abstract\r\n",
    "      if isinstance(abstract, list):\r\n",
    "            abstract_ = \"\"\r\n",
    "            # print(\"ordered dictionary is the abstract\")\r\n",
    "            try:\r\n",
    "               for row in abstract:\r\n",
    "                  for key , value in (row.items()):\r\n",
    "                        if (key == '#text'):\r\n",
    "                           abstract_ = abstract_ + value\r\n",
    "               text = abstract_\r\n",
    "               # print(\"Successufully parsed pubmed dictioanry file\")          \r\n",
    "            except :\r\n",
    "               print(\"error while parsing dictionary)\")\r\n",
    "      if isinstance(abstract, bytes):\r\n",
    "          print(pmid)\r\n",
    "          text = abstract.decode()\r\n",
    "      abstract = text\r\n",
    "      messages = [ {\"role\": \"system\", \"content\": 'Extract BEL Triples:\\n- Your task is to extract Biological Expression Language (BEL) triples from provided text.\\n- A BEL triple consists of three components: subject, relation, and object.\\nThe subject and object should be entities categorized into specific namespaces, such as MESH for diseases or biological processes, HGNC for genes, GO for gene functions, and CHEBI for chemicals.\\n- Each subject or object should be preceded by one of the BEL functions: abundance, activity, biologicalProcess, pathology, proteinAbundance, variant.\\n- The relation between subject and object should be chosen from the following list: increase, decrease, positiveCorrelation, negativeCorrelation, analogous, association, biomarkerFor, causesNoChange, directlyDecreases, directlyIncreases, hasActivity, hasComponent, hasComponents, hasMember, hasMembers, increases, isA, orthologous, prognosticBiomarkerFor, rateLimitingStepOf, regulates, subProcessOf, transcribedTo, translatedTo.\\n- Please provide the support evidence by including the sentence that contains the extracted BEL triple.\\n- Use  Namespaces to unambiguously reference concepts. \\n- Associate prefix HGNC for human loci, including protein coding genes, ncRNA genes and pseudogenes.\\n- Associate prefix CHEBI for molecular entities focused on small chemical compounds.\\n- Associate prefix GO to unify the representation of gene and gene product attributes across all species.\\n- Associate prefix MeSH for indexing articles for PubMed.\\n- Associate prefix miRBase which is a biological database that acts as an archive of microRNA sequences and annotations before miRNA names.\\n- Associate prefix MGI which is official gene symbol for a mouse gene.\\n- Associate prefix DO as a standardized ontology for human disease.\\n- Before each subject or object, use one of the BEL functions from the following: abundance, activity, biologicalProcess, pathology, proteinAbundance, variant.\\n\\nExample Triple:\\nTriple1:\\nsubject: biologicalProcess(GO:\"response to fluid shear stress\")\\nrelation: increases\\nobject: rnaAbundance(HGNC:NOS3)\\nsupport evidence: NO is generated by endothelial NO synthase (eNOS, or NOS3), the activity of which is increased by fluid shear stress through various mechanisms.\\n(Note: Ensure that the extracted BEL triples adhere to the specified format and namespaces.)'} ]\r\n",
    "      time.sleep(2)\r\n",
    "      # temperature = 0  (using default)\r\n",
    "      top_p = 0.1\r\n",
    "      message = \"Extract all BEL triples for each sentence and provide support evidence for each triple from the following :\" + abstract         \r\n",
    "      if message:\r\n",
    "         messages.append(\r\n",
    "         {\"role\": \"user\", \"content\": message},)\r\n",
    "         chat = openai.ChatCompletion.create(top_p = .1, model=\"gpt-4\", messages=messages, )\r\n",
    "         ''''''\r\n",
    "      #model=\"gpt-3.5-turbo\"\r\n",
    "      #model = \"gpt-4\"\r\n",
    "      reply = chat.choices[0].message.content\r\n",
    "      #print(reply)\r\n",
    "      file = open(\"gpt-sherpa-notprocessed-abstracts/GPT4/\"+str(pmid)+\".txt\", \"w+\", encoding='utf-8')\r\n",
    "      file.write(reply)\r\n",
    "      file.close()\r\n",
    "      messages.append({\"role\": \"assistant\", \"content\": reply})\r\n",
    "      #time.sleep(5)\r\n",
    "   except Exception as e:\r\n",
    "       print(e)\r\n",
    "\r\n",
    "   \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}